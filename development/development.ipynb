{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source code -> https://github.com/home-assistant/home-assistant/blob/dev/homeassistant/components/london_air/sensor.py\n",
    "\n",
    "API for hourly returns data in format (see `hourly.json`):\n",
    "```\n",
    "LocalAuthority (Borough)\n",
    "    Site\n",
    "        Species (CO2, NO2)\n",
    "```\n",
    "\n",
    "Not all Boroughs have sites, and generally each site might have a different combination of species. This makes it tedious to get the data into tabular format - we will require multiindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "import requests\n",
    "import pandas as pd\n",
    "from typing import List, Set, Dict, Tuple, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONF_LOCATIONS = \"locations\"\n",
    "\n",
    "ALL_AUTHORITIES = [\n",
    "    'Barking and Dagenham',\n",
    "     'Barnet',\n",
    "     'Bexley',\n",
    "     'Brent',\n",
    "     'Bromley',\n",
    "     'Camden',\n",
    "     'City of London',\n",
    "     'Croydon',\n",
    "     'Ealing',\n",
    "     'Enfield',\n",
    "     'Greenwich',\n",
    "     'Hackney',\n",
    "     'Hammersmith and Fulham',\n",
    "     'Haringey',\n",
    "     'Harrow',\n",
    "     'Havering',\n",
    "     'Hillingdon',\n",
    "     'Hounslow',\n",
    "     'Islington',\n",
    "     'Kensington and Chelsea',\n",
    "     'Kingston',\n",
    "     'Lambeth',\n",
    "     'Lewisham',\n",
    "     'Merton',\n",
    "     'Newham',\n",
    "     'Redbridge',\n",
    "     'Richmond',\n",
    "     'Southwark',\n",
    "     'Sutton',\n",
    "     'Tower Hamlets',\n",
    "     'Waltham Forest',\n",
    "     'Wandsworth',\n",
    "     'Westminster'\n",
    "]\n",
    "\n",
    "## Use only authorities with data, this might change over time\n",
    "AUTHORITIES = [\n",
    "    \"Barking and Dagenham\",\n",
    "    \"Bexley\",\n",
    "    \"Brent\",\n",
    "    \"Camden\",\n",
    "    \"City of London\",\n",
    "    \"Croydon\",\n",
    "    \"Ealing\",\n",
    "    \"Enfield\",\n",
    "    \"Greenwich\",\n",
    "    \"Hackney\",\n",
    "    \"Haringey\",\n",
    "    \"Harrow\",\n",
    "    \"Havering\",\n",
    "    \"Hillingdon\",\n",
    "    \"Islington\",\n",
    "    \"Kensington and Chelsea\",\n",
    "    \"Kingston\",\n",
    "    \"Lambeth\",\n",
    "    \"Lewisham\",\n",
    "    \"Merton\",\n",
    "    \"Redbridge\",\n",
    "    \"Richmond\",\n",
    "    \"Southwark\",\n",
    "    \"Sutton\",\n",
    "    \"Tower Hamlets\",\n",
    "    \"Wandsworth\",\n",
    "    \"Westminster\",\n",
    "]\n",
    "\n",
    "LAQ_HOURLY_URL = \"http://api.erg.kcl.ac.uk/AirQuality/Hourly/MonitoringIndex/GroupName=London/Json\"\n",
    "\n",
    "TIMEOUT = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(AUTHORITIES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper for making the request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_data(url : str, timeout : int = TIMEOUT) -> Dict:\n",
    "    \"\"\"\n",
    "    Request data from a URL and return valid data as dictionary.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=TIMEOUT)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            raise LondonAirQualityException(\n",
    "                f\"Status code {response.status_code} returned from {url}\")\n",
    "\n",
    "    except requests.exceptions.Timeout:\n",
    "        raise LondonAirQualityException(\n",
    "            f\"Request timeout, current timeout is {timeout} seconds\"\n",
    "        )\n",
    "\n",
    "    except requests.exceptions.ConnectionError as exc:\n",
    "        raise LondonAirQualityException(f\"Internet connection error: {exc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the functionality of this package is parsing this raw data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_species(species_data):\n",
    "    \"\"\"Iterate over list of species at each site.\"\"\"\n",
    "    parsed_species_data = []\n",
    "    quality_list = []\n",
    "    for species in species_data:\n",
    "        if species[\"@AirQualityBand\"] != \"No data\":\n",
    "            species_dict = {}\n",
    "            species_dict[\"description\"] = species[\"@SpeciesDescription\"]\n",
    "            species_dict[\"code\"] = species[\"@SpeciesCode\"]\n",
    "            species_dict[\"quality\"] = species[\"@AirQualityBand\"]\n",
    "            species_dict[\"index\"] = species[\"@AirQualityIndex\"]\n",
    "            species_dict[\"summary\"] = (\n",
    "                species_dict[\"code\"] + \" is \" + species_dict[\"quality\"]\n",
    "            )\n",
    "            parsed_species_data.append(species_dict)\n",
    "            quality_list.append(species_dict[\"quality\"])\n",
    "    return parsed_species_data, quality_list\n",
    "\n",
    "\n",
    "def parse_site(entry_sites_data):\n",
    "    \"\"\"Iterate over all sites at an local authority and tidy the data.\"\"\"\n",
    "    authority_data = []\n",
    "    for site in entry_sites_data:\n",
    "        site_data = {}\n",
    "        species_data = []\n",
    "\n",
    "        site_data[\"updated\"] = site[\"@BulletinDate\"]\n",
    "        site_data[\"latitude\"] = site[\"@Latitude\"]\n",
    "        site_data[\"longitude\"] = site[\"@Longitude\"]\n",
    "        site_data[\"site_code\"] = site[\"@SiteCode\"]\n",
    "        site_data[\"site_name\"] = site[\"@SiteName\"].split(\"-\")[-1].lstrip()\n",
    "        site_data[\"site_type\"] = site[\"@SiteType\"]\n",
    "\n",
    "        if isinstance(site[\"Species\"], dict):\n",
    "            species_data = [site[\"Species\"]]\n",
    "        else:\n",
    "            species_data = site[\"Species\"]\n",
    "\n",
    "        parsed_species_data, quality_list = parse_species(species_data)\n",
    "\n",
    "        if not parsed_species_data:\n",
    "            parsed_species_data.append(\"no_species_data\")\n",
    "        site_data[\"pollutants\"] = parsed_species_data\n",
    "\n",
    "        if quality_list:\n",
    "            site_data[\"pollutants_status\"] = max(\n",
    "                set(quality_list), key=quality_list.count\n",
    "            )\n",
    "            site_data[\"number_of_pollutants\"] = len(quality_list)\n",
    "        else:\n",
    "            site_data[\"pollutants_status\"] = \"no_species_data\"\n",
    "            site_data[\"number_of_pollutants\"] = 0\n",
    "\n",
    "        authority_data.append(site_data)\n",
    "    return authority_data\n",
    "\n",
    "\n",
    "def parse_hourly_response(hourly_response : Dict) -> Dict:\n",
    "    \"\"\"Return data indexed by Borough.\"\"\"\n",
    "    data = dict.fromkeys(AUTHORITIES)\n",
    "    for authority in AUTHORITIES:\n",
    "        try:\n",
    "            for entry in hourly_response[\"HourlyAirQualityIndex\"][\"LocalAuthority\"]:\n",
    "                if entry[\"@LocalAuthorityName\"] == authority:\n",
    "\n",
    "                    if isinstance(entry[\"Site\"], dict):\n",
    "                        entry_sites_data = [entry[\"Site\"]]\n",
    "                    else:\n",
    "                        entry_sites_data = entry[\"Site\"]\n",
    "\n",
    "                    data[authority] = parse_site(entry_sites_data)\n",
    "        except Exception as exc: \n",
    "            # catch misformatted or missing data\n",
    "            print(exc)\n",
    "            data[authority] = {}\n",
    "    return data\n",
    "\n",
    "class LondonAirQualityException(Exception):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    hourly_data_raw = request_data(LAQ_HOURLY_URL)\n",
    "except LondonAirQualityException as exc:\n",
    "    print(exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hourly_data_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The API response lists 33 `LocalAuthority` but only 27 contain actual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hourly_data_raw['HourlyAirQualityIndex']['LocalAuthority'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the raw data for a single site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'@LocalAuthorityCode': '1',\n",
       " '@LocalAuthorityName': 'Barking and Dagenham',\n",
       " '@LaCentreLatitude': '51.538435',\n",
       " '@LaCentreLongitude': '0.11467',\n",
       " '@LaCentreLatitudeWGS84': '6717095.01808',\n",
       " '@LaCentreLongitudeWGS84': '12765.0060093',\n",
       " 'Site': [{'@BulletinDate': '2019-09-26 07:00:00',\n",
       "   '@SiteCode': 'BG1',\n",
       "   '@SiteName': 'Barking and Dagenham - Rush Green',\n",
       "   '@SiteType': 'Suburban',\n",
       "   '@Latitude': '51.563752',\n",
       "   '@Longitude': '0.177891',\n",
       "   '@LatitudeWGS84': '6721627.34498',\n",
       "   '@LongitudeWGS84': '19802.7355367',\n",
       "   '@OwnerID': '1',\n",
       "   'Species': [{'@SpeciesCode': 'NO2',\n",
       "     '@SpeciesDescription': 'Nitrogen Dioxide',\n",
       "     '@AirQualityIndex': '1',\n",
       "     '@AirQualityBand': 'Low',\n",
       "     '@IndexSource': 'Measurement'},\n",
       "    {'@SpeciesCode': 'SO2',\n",
       "     '@SpeciesDescription': 'Sulphur Dioxide',\n",
       "     '@AirQualityIndex': '1',\n",
       "     '@AirQualityBand': 'Low',\n",
       "     '@IndexSource': 'Measurement'}]},\n",
       "  {'@BulletinDate': '2019-09-26 07:00:00',\n",
       "   '@SiteCode': 'BG2',\n",
       "   '@SiteName': 'Barking and Dagenham - Scrattons Farm',\n",
       "   '@SiteType': 'Suburban',\n",
       "   '@Latitude': '51.529389',\n",
       "   '@Longitude': '0.132857',\n",
       "   '@LatitudeWGS84': '6715476.18683',\n",
       "   '@LongitudeWGS84': '14789.5735883',\n",
       "   '@OwnerID': '1',\n",
       "   'Species': [{'@SpeciesCode': 'NO2',\n",
       "     '@SpeciesDescription': 'Nitrogen Dioxide',\n",
       "     '@AirQualityIndex': '1',\n",
       "     '@AirQualityBand': 'Low',\n",
       "     '@IndexSource': 'Measurement'},\n",
       "    {'@SpeciesCode': 'PM10',\n",
       "     '@SpeciesDescription': 'PM10 Particulate',\n",
       "     '@AirQualityIndex': '1',\n",
       "     '@AirQualityBand': 'Low',\n",
       "     '@IndexSource': 'Trigger'}]}]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hourly_data_raw['HourlyAirQualityIndex']['LocalAuthority'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse out the sites data for this local authority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'updated': '2019-09-26 07:00:00',\n",
       "  'latitude': '51.563752',\n",
       "  'longitude': '0.177891',\n",
       "  'site_code': 'BG1',\n",
       "  'site_name': 'Rush Green',\n",
       "  'site_type': 'Suburban',\n",
       "  'pollutants': [{'description': 'Nitrogen Dioxide',\n",
       "    'code': 'NO2',\n",
       "    'quality': 'Low',\n",
       "    'index': '1',\n",
       "    'summary': 'NO2 is Low'},\n",
       "   {'description': 'Sulphur Dioxide',\n",
       "    'code': 'SO2',\n",
       "    'quality': 'Low',\n",
       "    'index': '1',\n",
       "    'summary': 'SO2 is Low'}],\n",
       "  'pollutants_status': 'Low',\n",
       "  'number_of_pollutants': 2},\n",
       " {'updated': '2019-09-26 07:00:00',\n",
       "  'latitude': '51.529389',\n",
       "  'longitude': '0.132857',\n",
       "  'site_code': 'BG2',\n",
       "  'site_name': 'Scrattons Farm',\n",
       "  'site_type': 'Suburban',\n",
       "  'pollutants': [{'description': 'Nitrogen Dioxide',\n",
       "    'code': 'NO2',\n",
       "    'quality': 'Low',\n",
       "    'index': '1',\n",
       "    'summary': 'NO2 is Low'},\n",
       "   {'description': 'PM10 Particulate',\n",
       "    'code': 'PM10',\n",
       "    'quality': 'Low',\n",
       "    'index': '1',\n",
       "    'summary': 'PM10 is Low'}],\n",
       "  'pollutants_status': 'Low',\n",
       "  'number_of_pollutants': 2}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sites_data = parse_site(hourly_data_raw['HourlyAirQualityIndex']['LocalAuthority'][0]['Site'])\n",
    "sites_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This authority has 2 sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sites_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first site is monitoring 2 pollutants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'description': 'Nitrogen Dioxide',\n",
       "  'code': 'NO2',\n",
       "  'quality': 'Low',\n",
       "  'index': '1',\n",
       "  'summary': 'NO2 is Low'},\n",
       " {'description': 'Sulphur Dioxide',\n",
       "  'code': 'SO2',\n",
       "  'quality': 'Low',\n",
       "  'index': '1',\n",
       "  'summary': 'SO2 is Low'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sites_data[0]['pollutants']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main funtion wraps up all these steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hourly_data_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_data = parse_hourly_response(hourly_data_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hourly_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'updated': '2019-09-26 07:00:00',\n",
       "  'latitude': '51.563752',\n",
       "  'longitude': '0.177891',\n",
       "  'site_code': 'BG1',\n",
       "  'site_name': 'Rush Green',\n",
       "  'site_type': 'Suburban',\n",
       "  'pollutants': [{'description': 'Nitrogen Dioxide',\n",
       "    'code': 'NO2',\n",
       "    'quality': 'Low',\n",
       "    'index': '1',\n",
       "    'summary': 'NO2 is Low'},\n",
       "   {'description': 'Sulphur Dioxide',\n",
       "    'code': 'SO2',\n",
       "    'quality': 'Low',\n",
       "    'index': '1',\n",
       "    'summary': 'SO2 is Low'}],\n",
       "  'pollutants_status': 'Low',\n",
       "  'number_of_pollutants': 2},\n",
       " {'updated': '2019-09-26 07:00:00',\n",
       "  'latitude': '51.529389',\n",
       "  'longitude': '0.132857',\n",
       "  'site_code': 'BG2',\n",
       "  'site_name': 'Scrattons Farm',\n",
       "  'site_type': 'Suburban',\n",
       "  'pollutants': [{'description': 'Nitrogen Dioxide',\n",
       "    'code': 'NO2',\n",
       "    'quality': 'Low',\n",
       "    'index': '1',\n",
       "    'summary': 'NO2 is Low'},\n",
       "   {'description': 'PM10 Particulate',\n",
       "    'code': 'PM10',\n",
       "    'quality': 'Low',\n",
       "    'index': '1',\n",
       "    'summary': 'PM10 is Low'}],\n",
       "  'pollutants_status': 'Low',\n",
       "  'number_of_pollutants': 2}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hourly_data['Barking and Dagenham']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
